{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "eryZlfmUnzvH"
   },
   "source": [
    "Pada notebook ini akan menerapkan model klasifikasi teks pada data Twitter menggunakan metode Naive Bayes Classifier untuk melakukan klasifikasi tweet dalam kategori prostitusi (True) ataupun bukan prostitusi (False).\n",
    "\n",
    "\n",
    "Dataset yang dimasukkan disini meliputi data training dan data testing. Ada 40000 data training yang dibagi menjadi dua bagian, yaitu data 20000 True (prostitusi) dan 20000 data False (bukan prostitusi), dan 10000 data sebagai data testing.\n",
    "\n",
    "Pengujian dilakukan untuk menentukan akurasi klasifikasi metode NBC dan menggunakan Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSP8HsN3nzvI"
   },
   "source": [
    "Langkah pertama, persiapkan module python yang akan digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Luf19yynzvJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULgQjnjcnzvN"
   },
   "source": [
    "Import data training dan data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_Jaoee3nzvO"
   },
   "outputs": [],
   "source": [
    "data_train_true = pd.read_excel('twitter-prostitute.xlsx')\n",
    "data_train_false = pd.read_excel('twitter-not-prostitute.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "1c0Xiq3HnzvR",
    "outputId": "258221e0-ba42-4afd-bcbf-49a922e2a7fd"
   },
   "outputs": [],
   "source": [
    "# Gabungkan semua dataset\n",
    "dataset = pd.concat([data_train_true, data_train_false], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Lmsaf6sZnzvU",
    "outputId": "5ec9483d-bb27-4378-a164-5930b44b0116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                 date       username  \\\n",
      "0  1255052854739398658  2020-04-28 00:00:00   rina11091996   \n",
      "1  1255052799798202373  2020-04-28 00:00:00   rina11091996   \n",
      "2  1255052613646573569  2020-04-28 00:00:00  viollasyantik   \n",
      "3  1255052558667661312  2020-04-28 00:00:00    lyannyhijab   \n",
      "4  1255052557061287938  2020-04-28 00:00:00      dheajogja   \n",
      "5  1255052446486761481  2020-04-28 00:00:00   ayudewijogja   \n",
      "6  1255052156417146880  2020-04-28 00:00:00   alexabojogja   \n",
      "7  1255051841395503104  2020-04-28 00:00:00     alexayolan   \n",
      "8  1255051002836738048  2020-04-28 00:00:00     nanabellen   \n",
      "9  1255049688308428800  2020-04-28 00:00:00  realacoount18   \n",
      "\n",
      "                                               tweet  status  \n",
      "0  AvaiL BO yaa bebüòô\\nWA 0831 9315 9762\\n#AvailJo...       1  \n",
      "1  Include exclude Ready beb \\nWa 0831 9315 9762\\...       1  \n",
      "2  AvaiL Jogja Minat DM ajaüòç\\nFasht Respon.\\n#Ava...       1  \n",
      "3  MAEN SANTAI GA BURU\" \\nFULL SERVICE NO ANAL US...       1  \n",
      "4  New bie...Ready ya..2 slot aja 085647266101\\n#...       1  \n",
      "5  New bie...Ready ya..2 slot aja 085641953922\\n#...       1  \n",
      "6  Open Massage +\\nPijat mantaps\\nInc/exc hotel j...       1  \n",
      "7  Open Massage +\\nPijat mantaps\\nInc/exc hotel j...       1  \n",
      "8  Open bo no DP \\nwa 085943535739\\nKulit bersih,...       1  \n",
      "9  Jangan Lupa Bo,full bonusüëô\\nWa 089661645252\\n#...       1  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "gNmNfxcWnzvX",
    "outputId": "90522d77-cd89-420b-ebd1-8ddea353863b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    AvaiL BO yaa bebüòô\\nWA 0831 9315 9762\\n#AvailJo...\n",
      "1    Include exclude Ready beb \\nWa 0831 9315 9762\\...\n",
      "2    AvaiL Jogja Minat DM ajaüòç\\nFasht Respon.\\n#Ava...\n",
      "3    MAEN SANTAI GA BURU\" \\nFULL SERVICE NO ANAL US...\n",
      "4    New bie...Ready ya..2 slot aja 085647266101\\n#...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset['tweet'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WusfmB0knzvb",
    "outputId": "7ff19453-0904-4039-f2e0-54ad95b1dcd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.loc[dataset['status']==1]))\n",
    "print(len(dataset.loc[dataset['status']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ep9oGAWPnzvf"
   },
   "source": [
    "Setelah data berhasil dibuka, saatnya melakukan preprocessing pada teks. Yang akan dilakukan preprocessing yaitu data pada kolom tweet dimana data ini berisi tweet yang diambil dari twitter.\n",
    "\n",
    "Ada beberapa process preprocessing yaitu cleaning text atau membersihkan text dari noise dan tokenizing yaitu memecah semua text menjadi per kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4LIhSMsnzvi"
   },
   "outputs": [],
   "source": [
    "stopwords_file = open(\"stopwords-id.txt\", 'r')\n",
    "stopwords = [x.strip() for x in stopwords_file.readlines()]\n",
    "stopwords.extend(['by', 'rt', 'via'])\n",
    "\n",
    "def cleaning(text):\n",
    "\ttext = re.sub(r'<[^>]+>', '', text) #delete html tags\n",
    "\ttext = re.sub(r'\\S*twitter.com\\S*', '', text)   #delete twitter image\n",
    "\ttext = re.sub(r'https?://[A-Za-z0-9./]+','',text) #delete url\n",
    "\ttext = re.sub(r'@[A-Za-z0-9]+','',text) #delete user mention\n",
    "\ttext = re.sub(r'#[A-Za-z0-9]+','',text) #delete twitter hashtag\n",
    "\ttext = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)','', text) #delete number\n",
    "\ttext = re.sub(r\"[^a-zA-Z]\", \" \", text) #only accept alphabet char\n",
    "\ttext = re.sub(r\"(\\w)(\\1{2,})\", r'\\1', text) #delete repeated char\n",
    "\ttext = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text) #remove single character\n",
    "\ttext = text.lower() #change to lowercase\n",
    "\treturn text\n",
    "\n",
    "def tokenize(text):\n",
    "\t#disini diisi dengan stop words\n",
    "\twords = text.split();\n",
    "\twords = [w for w in words if w not in stopwords]\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1DWaenJnzvp"
   },
   "source": [
    "Cleaning text pada data tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mham-KkWcPTW"
   },
   "outputs": [],
   "source": [
    "dataset['tweet'] = dataset.tweet.map(lambda x: cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbCaRUFunzvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             avail bo yaa beb  wa                    \n",
      "1    include exclude ready beb  wa                 ...\n",
      "2    avail jogja minat dm aja  fasht respon        ...\n",
      "3    maen santai ga buru   full service no anal use...\n",
      "4      new bie   ready ya   slot aja                  \n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset['tweet'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCO6QvGJnzvu"
   },
   "outputs": [],
   "source": [
    "dataset['tweet'] = dataset.tweet.apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8q1UrIGnzvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            [avail, bo, yaa, beb, wa]\n",
      "1                   [include, exclude, ready, beb, wa]\n",
      "2        [avail, jogja, minat, dm, aja, fasht, respon]\n",
      "3    [maen, santai, ga, buru, full, service, no, an...\n",
      "4                     [new, bie, ready, ya, slot, aja]\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset['tweet'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM8px4EEnzv0"
   },
   "source": [
    "Menggabungkan semua tweet menggunakan spasi untuk dilakukan tahap selanjutnya yaitu pembuatan vektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKG2xPIbnzv0"
   },
   "outputs": [],
   "source": [
    "dataset['tweet'] = dataset.tweet.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cruX4p2Inzv5"
   },
   "source": [
    "Memasukkan label ke array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "hfc4q5Qvnzv6",
    "outputId": "bb7f7115-1f3b-473f-e8d4-876fcb771330"
   },
   "outputs": [],
   "source": [
    "dataset['label'] = dataset.status.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "oXp5PMrIdLLU",
    "outputId": "6707fcf9-249a-48aa-de4d-a23a8d57463b"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(dataset['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GAICyEdKnzv9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1715)\t1\n",
      "  (0, 4248)\t1\n",
      "  (0, 37085)\t1\n",
      "  (0, 2486)\t1\n",
      "  (0, 36375)\t1\n",
      "  (1, 2486)\t1\n",
      "  (1, 36375)\t1\n",
      "  (1, 13185)\t1\n",
      "  (1, 9869)\t1\n",
      "  (1, 28573)\t1\n",
      "  (2, 1715)\t1\n",
      "  (2, 14328)\t1\n",
      "  (2, 21932)\t1\n",
      "  (2, 8839)\t1\n",
      "  (2, 466)\t1\n",
      "  (2, 10073)\t1\n",
      "  (2, 28935)\t1\n",
      "  (3, 18837)\t1\n",
      "  (3, 29840)\t1\n",
      "  (3, 10628)\t1\n",
      "  (3, 4838)\t1\n",
      "  (3, 10577)\t1\n",
      "  (3, 31068)\t1\n",
      "  (3, 23900)\t1\n",
      "  (3, 975)\t1\n",
      "  (3, 36000)\t1\n",
      "  (3, 5072)\t1\n",
      "  (4, 28573)\t1\n",
      "  (4, 466)\t1\n",
      "  (4, 23215)\t1\n",
      "  (4, 3936)\t1\n",
      "  (4, 37084)\t1\n",
      "  (4, 31856)\t1\n"
     ]
    }
   ],
   "source": [
    "print(counts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEVPSnqunzwB"
   },
   "source": [
    "Disini mentransformasikan semua feature kata menggunakan TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "text",
    "id": "3vRHsw8LnzwE"
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "da-Mfm2OnzwE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 37085)\t0.5911705953129078\n",
      "  (0, 36375)\t0.2822194474686629\n",
      "  (0, 4248)\t0.3843499205847922\n",
      "  (0, 2486)\t0.47674485769290337\n",
      "  (0, 1715)\t0.44255958911507276\n",
      "  (1, 36375)\t0.2866682929106074\n",
      "  (1, 28573)\t0.38428108764986996\n",
      "  (1, 13185)\t0.49885978763514066\n",
      "  (1, 9869)\t0.5355187636154758\n",
      "  (1, 2486)\t0.4842601590165411\n",
      "  (2, 28935)\t0.3746459280373387\n",
      "  (2, 21932)\t0.2760387849741051\n",
      "  (2, 14328)\t0.21527389492118496\n",
      "  (2, 10073)\t0.7561577668564705\n",
      "  (2, 8839)\t0.18528478090139275\n",
      "  (2, 1715)\t0.2457130613228591\n",
      "  (2, 466)\t0.2657446914706499\n",
      "  (3, 36000)\t0.38496965117902543\n",
      "  (3, 31068)\t0.322521532751217\n",
      "  (3, 29840)\t0.2868751168311405\n",
      "  (3, 23900)\t0.17759626056082173\n",
      "  (3, 18837)\t0.3707445553998642\n",
      "  (3, 10628)\t0.2008833216432625\n",
      "  (3, 10577)\t0.2868751168311405\n",
      "  (3, 5072)\t0.34646183022969057\n",
      "  (3, 4838)\t0.3628687482176093\n",
      "  (3, 975)\t0.34946812299802377\n",
      "  (4, 37084)\t0.2176563413269418\n",
      "  (4, 31856)\t0.28790259324420076\n",
      "  (4, 28573)\t0.23764296132175766\n",
      "  (4, 23215)\t0.44937909480755067\n",
      "  (4, 3936)\t0.7217514546474594\n",
      "  (4, 466)\t0.30066077842293026\n"
     ]
    }
   ],
   "source": [
    "print(counts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nAMBVI6Zucq7",
    "outputId": "4d3ee6a5-828e-4671-d141-bdf22e89f13c"
   },
   "source": [
    "Kemudian setelah semua kata memiliki bobot tersendiri sesuai dengan transformasi vektor TF IDF, disini pembagian data training dan data testing dengan alokasi 80% train:20% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNvbPRecnzwG"
   },
   "source": [
    "Gabungkan semua daftar kata dalam sebuah list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAt0K-ybnzwH"
   },
   "outputs": [],
   "source": [
    "feature_train, feature_test, target_train, target_test = train_test_split(counts, dataset['label'], \n",
    "    train_size=0.8, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NaX_n1nnzwJ"
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(feature_train, target_train)\n",
    "predicted = model.predict(feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNwfX7uQnzwL"
   },
   "source": [
    "Untuk menghitung akurasi model yang sudah dibuat, dapat menggunakan accuracy_score dari scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H76P2RAonzwL"
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(target_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2w6TAgjnaYW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985125\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk pengujian, menggunakan confusion matrix sebagai berikut ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qPX3wCHBnzwP",
    "outputId": "d1320ba4-b4cc-446c-dd1d-0b2cd9b0532e"
   },
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(target_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3Zd-TxmnzwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4009   71]\n",
      " [  48 3872]]\n"
     ]
    }
   ],
   "source": [
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbnk_yxqnzwT"
   },
   "source": [
    "Untuk melihat laporan klasifikasi, menggunakan classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "E_2qMuO1nzwT",
    "outputId": "fb0dce7c-dc48-4fd4-e616-256a343741cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      4080\n",
      "           1       0.98      0.99      0.98      3920\n",
      "\n",
      "    accuracy                           0.99      8000\n",
      "   macro avg       0.99      0.99      0.99      8000\n",
      "weighted avg       0.99      0.99      0.99      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_report = classification_report(target_test, predicted)\n",
    "print(c_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_text(score):\n",
    "  if score == 0:\n",
    "    return 'False (Not Prostitute)'\n",
    "  else:\n",
    "    return 'True (Prostitute)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False (Not Prostitute)\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Semoga covid-19 ini segera berakhir dan semua kegiatan dapat kembali seperti sedia kala'\n",
    "input_text = tokenize(cleaning(input_text))\n",
    "new_counts = count_vect.transform(input_text)\n",
    "pred = model.predict(new_counts)\n",
    "print(pred_text(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True (Prostitute)\n"
     ]
    }
   ],
   "source": [
    "input_text = 'area #Jogja cod no dp free cancel include room'\n",
    "input_text = tokenize(cleaning(input_text))\n",
    "new_counts = count_vect.transform(input_text)\n",
    "pred = model.predict(new_counts)\n",
    "print(pred_text(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99084375\n",
      "0.985125\n"
     ]
    }
   ],
   "source": [
    "print(model.score(feature_train, target_train))\n",
    "print(model.score(feature_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9871875 0.9834375 0.9871875 0.9875    0.981875  0.9884375 0.9884375\n",
      " 0.9846875 0.9859375 0.9865625]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, feature_train, target_train, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Tweet Classification for Classify Online Prostitut.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
